{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "assert len(physical_devices) > 0, \"Not enough GPU hardware devices available\"\n",
    "config = tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'BinanceLOB/binance_dataset_2021-01-16_2194535408.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lob_list = []\n",
    "\n",
    "for side in ['bid','ask']:\n",
    "    for i in range(100):\n",
    "        lob_list.append(side + 'price' + str(i+1))\n",
    "        lob_list.append(side + 'size' + str(i+1))\n",
    "header_list = ['timestamp','last'];\n",
    "header_list.extend(lob_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "binance_ethbtc = pd.read_csv(data_path, names=header_list, index_col='timestamp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_lob_order = []\n",
    "for i in range(100):\n",
    "    for side in ['bid','ask']:\n",
    "        new_lob_order.append(side + 'price' + str(i+1))\n",
    "        new_lob_order.append(side + 'size' + str(i+1))\n",
    "new_lob_order = ['last'] + new_lob_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "binance_ethbtc = binance_ethbtc[new_lob_order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_data(data, sample_size=600, feature_num=200):\n",
    "    data = data.values\n",
    "    shape = data.shape\n",
    "    X = np.zeros((shape[0]-sample_size, sample_size, feature_num))\n",
    "    Y = np.zeros(shape=(shape[0]-sample_size, 1))\n",
    "    for i in range(shape[0]-sample_size):\n",
    "        X[i] = data[i:i+sample_size,0:feature_num]# take the first feature_num columns as features\n",
    "        Y[i] = data[i+sample_size-1,-1:]# take the last one column as labels\n",
    "    X = X.reshape(X.shape[0], sample_size, feature_num, 1)# add the 4th dimension: 1 channel\n",
    "    \n",
    "    return X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_y_labels(y, prediction_period, band_size = 0.001):\n",
    "    bins = [-np.inf, -band_size, band_size, np.inf]\n",
    "    names = [0, 1, 2]\n",
    "    y_labels = pd.cut(y.pct_change(periods=prediction_period), bins, labels=names)\n",
    "    return y_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasample_period = 600\n",
    "feature_columns = 40\n",
    "prediction_period = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "binance_ethbtc['delta_cat'] = define_y_labels(binance_ethbtc['last'], prediction_period)\n",
    "weight_array = binance_ethbtc.groupby('delta_cat').count()['last']\n",
    "weight_sum = weight_array.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weight = {0: 1- weight_array[0]/weight_sum, 1: 1- weight_array[1]/weight_sum, 2: 1-weight_array[2]/weight_sum}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split training and validation dataset\n",
    "train_X, train_Y = get_model_data(binance_ethbtc.drop(['last'], axis=1), datasample_period, feature_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = to_categorical(train_Y[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 600, 40, 1)] 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 600, 20, 16)  48          input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)         (None, 600, 20, 16)  0           conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 600, 20, 16)  1040        leaky_re_lu[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)       (None, 600, 20, 16)  0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 600, 20, 16)  1040        leaky_re_lu_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)       (None, 600, 20, 16)  0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 600, 10, 16)  528         leaky_re_lu_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)       (None, 600, 10, 16)  0           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 600, 10, 16)  1040        leaky_re_lu_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)       (None, 600, 10, 16)  0           conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 600, 10, 16)  1040        leaky_re_lu_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)       (None, 600, 10, 16)  0           conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 600, 1, 16)   2576        leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)       (None, 600, 1, 16)   0           conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 600, 1, 16)   1040        leaky_re_lu_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)       (None, 600, 1, 16)   0           conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 600, 1, 16)   1040        leaky_re_lu_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)       (None, 600, 1, 16)   0           conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 600, 1, 32)   544         leaky_re_lu_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 600, 1, 32)   544         leaky_re_lu_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)       (None, 600, 1, 32)   0           conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_11 (LeakyReLU)      (None, 600, 1, 32)   0           conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 600, 1, 16)   0           leaky_re_lu_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 600, 1, 32)   3104        leaky_re_lu_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 600, 1, 32)   5152        leaky_re_lu_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 600, 1, 32)   544         max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)      (None, 600, 1, 32)   0           conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_12 (LeakyReLU)      (None, 600, 1, 32)   0           conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_13 (LeakyReLU)      (None, 600, 1, 32)   0           conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 600, 1, 96)   0           leaky_re_lu_10[0][0]             \n",
      "                                                                 leaky_re_lu_12[0][0]             \n",
      "                                                                 leaky_re_lu_13[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "reshape (Reshape)               (None, 600, 96)      0           concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     (None, 64)           41216       reshape[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 3)            195         lstm[0][0]                       \n",
      "==================================================================================================\n",
      "Total params: 60,691\n",
      "Trainable params: 60,691\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# the size of a single input is (600,200)\n",
    "input_tensor = Input(shape=(datasample_period,feature_columns,1))\n",
    "\n",
    "# convolutional filter is (1,2) with stride of (1,2)\n",
    "layer_x = layers.Conv2D(16, (1,2), strides=(1,2))(input_tensor)\n",
    "layer_x = layers.LeakyReLU(alpha=0.01)(layer_x)\n",
    "layer_x = layers.Conv2D(16, (4,1), padding='same')(layer_x)\n",
    "layer_x = layers.LeakyReLU(alpha=0.01)(layer_x)\n",
    "layer_x = layers.Conv2D(16, (4,1), padding='same')(layer_x)\n",
    "layer_x = layers.LeakyReLU(alpha=0.01)(layer_x)\n",
    "\n",
    "layer_x = layers.Conv2D(16, (1,2), strides=(1,2))(layer_x)\n",
    "layer_x = layers.LeakyReLU(alpha=0.01)(layer_x)\n",
    "layer_x = layers.Conv2D(16, (4,1), padding='same')(layer_x)\n",
    "layer_x = layers.LeakyReLU(alpha=0.01)(layer_x)\n",
    "layer_x = layers.Conv2D(16, (4,1), padding='same')(layer_x)\n",
    "layer_x = layers.LeakyReLU(alpha=0.01)(layer_x)\n",
    "\n",
    "layer_x = layers.Conv2D(16, (1,10))(layer_x)\n",
    "layer_x = layers.LeakyReLU(alpha=0.01)(layer_x)\n",
    "layer_x = layers.Conv2D(16, (4,1), padding='same')(layer_x)\n",
    "layer_x = layers.LeakyReLU(alpha=0.01)(layer_x)\n",
    "layer_x = layers.Conv2D(16, (4,1), padding='same')(layer_x)\n",
    "layer_x = layers.LeakyReLU(alpha=0.01)(layer_x)\n",
    "\n",
    "# Inception Module\n",
    "tower_1 = layers.Conv2D(32, (1,1), padding='same')(layer_x)\n",
    "tower_1 = layers.LeakyReLU(alpha=0.01)(tower_1)\n",
    "tower_1 = layers.Conv2D(32, (3,1), padding='same')(tower_1)\n",
    "tower_1 = layers.LeakyReLU(alpha=0.01)(tower_1)\n",
    "\n",
    "tower_2 = layers.Conv2D(32, (1,1), padding='same')(layer_x)\n",
    "tower_2 = layers.LeakyReLU(alpha=0.01)(tower_2)\n",
    "tower_2 = layers.Conv2D(32, (5,1), padding='same')(tower_2)\n",
    "tower_2 = layers.LeakyReLU(alpha=0.01)(tower_2)  \n",
    "\n",
    "tower_3 = layers.MaxPooling2D((3,1), padding='same', strides=(1,1))(layer_x)\n",
    "tower_3 = layers.Conv2D(32, (1,1), padding='same')(tower_3)\n",
    "tower_3 = layers.LeakyReLU(alpha=0.01)(tower_3)\n",
    "\n",
    "layer_x = layers.concatenate([tower_1, tower_2, tower_3], axis=-1)\n",
    "\n",
    "# concatenate features of tower_1, tower_2, tower_3\n",
    "layer_x = layers.Reshape((600,96))(layer_x)\n",
    "\n",
    "# 64 LSTM units\n",
    "layer_x = LSTM(64)(layer_x)\n",
    "# The last output layer uses a softmax activation function\n",
    "output = layers.Dense(3, activation='softmax')(layer_x)\n",
    "model = Model(input_tensor, output)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6602 samples\n",
      "Epoch 1/100\n",
      "6602/6602 [==============================] - 34s 5ms/sample - loss: 0.5231 - accuracy: 0.6801\n",
      "Epoch 2/100\n",
      "6602/6602 [==============================] - 29s 4ms/sample - loss: 0.4858 - accuracy: 0.6721\n",
      "Epoch 3/100\n",
      "6602/6602 [==============================] - 29s 4ms/sample - loss: 0.4141 - accuracy: 0.6616\n",
      "Epoch 4/100\n",
      "6602/6602 [==============================] - 28s 4ms/sample - loss: 0.3598 - accuracy: 0.6990\n",
      "Epoch 5/100\n",
      "6602/6602 [==============================] - 28s 4ms/sample - loss: 0.3439 - accuracy: 0.7008\n",
      "Epoch 6/100\n",
      "6602/6602 [==============================] - 28s 4ms/sample - loss: 0.3095 - accuracy: 0.7384\n",
      "Epoch 7/100\n",
      "6602/6602 [==============================] - 29s 4ms/sample - loss: 0.2813 - accuracy: 0.7663\n",
      "Epoch 8/100\n",
      "6602/6602 [==============================] - 28s 4ms/sample - loss: 0.2555 - accuracy: 0.7958\n",
      "Epoch 9/100\n",
      "6602/6602 [==============================] - 29s 4ms/sample - loss: 0.2367 - accuracy: 0.8087\n",
      "Epoch 10/100\n",
      "6602/6602 [==============================] - 28s 4ms/sample - loss: 0.2355 - accuracy: 0.8129\n",
      "Epoch 11/100\n",
      "6602/6602 [==============================] - 29s 4ms/sample - loss: 0.2177 - accuracy: 0.8264\n",
      "Epoch 12/100\n",
      "6602/6602 [==============================] - 29s 4ms/sample - loss: 0.2348 - accuracy: 0.8026\n",
      "Epoch 13/100\n",
      "6602/6602 [==============================] - 29s 4ms/sample - loss: 0.2010 - accuracy: 0.8361\n",
      "Epoch 14/100\n",
      "6602/6602 [==============================] - 28s 4ms/sample - loss: 0.1869 - accuracy: 0.8457\n",
      "Epoch 15/100\n",
      "6602/6602 [==============================] - 28s 4ms/sample - loss: 0.1890 - accuracy: 0.8382\n",
      "Epoch 16/100\n",
      "6602/6602 [==============================] - 29s 4ms/sample - loss: 0.1681 - accuracy: 0.8582\n",
      "Epoch 17/100\n",
      "6602/6602 [==============================] - 28s 4ms/sample - loss: 0.1549 - accuracy: 0.8732\n",
      "Epoch 18/100\n",
      "6602/6602 [==============================] - 28s 4ms/sample - loss: 0.1579 - accuracy: 0.8726\n",
      "Epoch 19/100\n",
      "6602/6602 [==============================] - 28s 4ms/sample - loss: 0.1632 - accuracy: 0.8643\n",
      "Epoch 20/100\n",
      "6602/6602 [==============================] - 28s 4ms/sample - loss: 0.1540 - accuracy: 0.8723\n",
      "Epoch 21/100\n",
      "6602/6602 [==============================] - 28s 4ms/sample - loss: 0.2653 - accuracy: 0.7802\n",
      "Epoch 22/100\n",
      "6602/6602 [==============================] - 28s 4ms/sample - loss: 0.2534 - accuracy: 0.7984\n",
      "Epoch 23/100\n",
      "6602/6602 [==============================] - 28s 4ms/sample - loss: 0.2120 - accuracy: 0.8311\n",
      "Epoch 24/100\n",
      "6602/6602 [==============================] - 29s 4ms/sample - loss: 0.1996 - accuracy: 0.8400\n",
      "Epoch 25/100\n",
      "6602/6602 [==============================] - 29s 4ms/sample - loss: 0.1745 - accuracy: 0.8588\n",
      "Epoch 26/100\n",
      "6602/6602 [==============================] - 29s 4ms/sample - loss: 0.1749 - accuracy: 0.8603\n",
      "Epoch 27/100\n",
      "6602/6602 [==============================] - 29s 4ms/sample - loss: 0.1693 - accuracy: 0.8605\n",
      "Epoch 28/100\n",
      "6602/6602 [==============================] - 29s 4ms/sample - loss: 0.1467 - accuracy: 0.8794\n",
      "Epoch 29/100\n",
      "6602/6602 [==============================] - 28s 4ms/sample - loss: 0.1509 - accuracy: 0.8740\n",
      "Epoch 30/100\n",
      "6602/6602 [==============================] - 29s 4ms/sample - loss: 0.1456 - accuracy: 0.8762\n",
      "Epoch 31/100\n",
      "6602/6602 [==============================] - 29s 4ms/sample - loss: 0.1416 - accuracy: 0.8835\n",
      "Epoch 32/100\n",
      "6602/6602 [==============================] - 29s 4ms/sample - loss: 0.1248 - accuracy: 0.8975\n",
      "Epoch 33/100\n",
      "6602/6602 [==============================] - 29s 4ms/sample - loss: 0.1188 - accuracy: 0.9088\n",
      "Epoch 34/100\n",
      "6602/6602 [==============================] - 29s 4ms/sample - loss: 0.1222 - accuracy: 0.8984\n",
      "Epoch 35/100\n",
      "6602/6602 [==============================] - 28s 4ms/sample - loss: 0.1095 - accuracy: 0.9114\n",
      "Epoch 36/100\n",
      "6602/6602 [==============================] - 29s 4ms/sample - loss: 0.1259 - accuracy: 0.8990\n",
      "Epoch 37/100\n",
      "6602/6602 [==============================] - 29s 4ms/sample - loss: 0.1252 - accuracy: 0.8979\n",
      "Epoch 38/100\n",
      "6602/6602 [==============================] - 29s 4ms/sample - loss: 0.1400 - accuracy: 0.8809\n",
      "Epoch 39/100\n",
      "6602/6602 [==============================] - 29s 4ms/sample - loss: 0.1125 - accuracy: 0.9100\n",
      "Epoch 40/100\n",
      "6602/6602 [==============================] - 29s 4ms/sample - loss: 0.1081 - accuracy: 0.9106\n",
      "Epoch 41/100\n",
      "6602/6602 [==============================] - 29s 4ms/sample - loss: 0.0995 - accuracy: 0.9181\n",
      "Epoch 42/100\n",
      "6602/6602 [==============================] - 29s 4ms/sample - loss: 0.0939 - accuracy: 0.9237\n",
      "Epoch 43/100\n",
      "6602/6602 [==============================] - 29s 4ms/sample - loss: 0.0882 - accuracy: 0.9302\n",
      "Epoch 44/100\n",
      "6602/6602 [==============================] - 29s 4ms/sample - loss: 0.0908 - accuracy: 0.9249\n",
      "Epoch 45/100\n",
      "6602/6602 [==============================] - 29s 4ms/sample - loss: 0.0939 - accuracy: 0.9235\n",
      "Epoch 46/100\n",
      "6602/6602 [==============================] - 29s 4ms/sample - loss: 0.0892 - accuracy: 0.9265\n",
      "Epoch 47/100\n",
      "6602/6602 [==============================] - 29s 4ms/sample - loss: 0.0931 - accuracy: 0.9200\n",
      "Epoch 48/100\n",
      "6602/6602 [==============================] - 29s 4ms/sample - loss: 0.0991 - accuracy: 0.9224\n",
      "Epoch 49/100\n",
      "6602/6602 [==============================] - 29s 4ms/sample - loss: 0.0880 - accuracy: 0.9264\n",
      "Epoch 50/100\n",
      "6602/6602 [==============================] - 29s 4ms/sample - loss: 0.0844 - accuracy: 0.9335\n",
      "Epoch 51/100\n",
      "6602/6602 [==============================] - 29s 4ms/sample - loss: 0.0813 - accuracy: 0.9327\n",
      "Epoch 52/100\n",
      "6602/6602 [==============================] - 29s 4ms/sample - loss: 0.0810 - accuracy: 0.9367\n",
      "Epoch 53/100\n",
      "6602/6602 [==============================] - 29s 4ms/sample - loss: 0.0836 - accuracy: 0.9350\n",
      "Epoch 54/100\n",
      "6602/6602 [==============================] - 29s 4ms/sample - loss: 0.0789 - accuracy: 0.9364\n",
      "Epoch 55/100\n",
      "6602/6602 [==============================] - 29s 4ms/sample - loss: 0.0819 - accuracy: 0.9355\n",
      "Epoch 56/100\n",
      "6602/6602 [==============================] - 29s 4ms/sample - loss: 0.0760 - accuracy: 0.9405\n",
      "Epoch 57/100\n",
      "6602/6602 [==============================] - 29s 4ms/sample - loss: 0.0757 - accuracy: 0.9362\n",
      "Epoch 58/100\n",
      "6602/6602 [==============================] - 29s 4ms/sample - loss: 0.0797 - accuracy: 0.9353\n",
      "Epoch 59/100\n",
      "6602/6602 [==============================] - 30s 5ms/sample - loss: 0.0814 - accuracy: 0.9358\n",
      "Epoch 60/100\n",
      "6602/6602 [==============================] - 30s 5ms/sample - loss: 0.0817 - accuracy: 0.9327\n",
      "Epoch 61/100\n",
      "6602/6602 [==============================] - 30s 5ms/sample - loss: 0.0798 - accuracy: 0.9356\n",
      "Epoch 62/100\n",
      "6602/6602 [==============================] - 30s 5ms/sample - loss: 0.0809 - accuracy: 0.9347\n",
      "Epoch 63/100\n",
      "6602/6602 [==============================] - 30s 4ms/sample - loss: 0.0850 - accuracy: 0.9320\n",
      "Epoch 64/100\n",
      "6602/6602 [==============================] - 30s 4ms/sample - loss: 0.0764 - accuracy: 0.9373\n",
      "Epoch 65/100\n",
      "6602/6602 [==============================] - 30s 4ms/sample - loss: 0.0717 - accuracy: 0.9441\n",
      "Epoch 66/100\n",
      "6602/6602 [==============================] - 29s 4ms/sample - loss: 0.0772 - accuracy: 0.9382\n",
      "Epoch 67/100\n",
      "6602/6602 [==============================] - 29s 4ms/sample - loss: 0.0867 - accuracy: 0.9276\n",
      "Epoch 68/100\n",
      "6602/6602 [==============================] - 29s 4ms/sample - loss: 0.1072 - accuracy: 0.9118\n",
      "Epoch 69/100\n",
      "6602/6602 [==============================] - 29s 4ms/sample - loss: 0.0756 - accuracy: 0.9406\n",
      "Epoch 70/100\n",
      "6602/6602 [==============================] - 29s 4ms/sample - loss: 0.0757 - accuracy: 0.9373\n",
      "Epoch 71/100\n",
      "6602/6602 [==============================] - 30s 4ms/sample - loss: 0.0689 - accuracy: 0.9426\n",
      "Epoch 72/100\n",
      "6602/6602 [==============================] - 29s 4ms/sample - loss: 0.0763 - accuracy: 0.9388\n",
      "Epoch 73/100\n",
      "6602/6602 [==============================] - 29s 4ms/sample - loss: 0.0672 - accuracy: 0.9461\n",
      "Epoch 74/100\n",
      "6602/6602 [==============================] - 29s 4ms/sample - loss: 0.0709 - accuracy: 0.9370\n",
      "Epoch 75/100\n",
      "6602/6602 [==============================] - 30s 4ms/sample - loss: 0.1247 - accuracy: 0.9017\n",
      "Epoch 76/100\n",
      "6602/6602 [==============================] - 29s 4ms/sample - loss: 0.0862 - accuracy: 0.9250\n",
      "Epoch 77/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6602/6602 [==============================] - 30s 5ms/sample - loss: 0.0701 - accuracy: 0.9423\n",
      "Epoch 78/100\n",
      "6602/6602 [==============================] - 29s 4ms/sample - loss: 0.0653 - accuracy: 0.9453\n",
      "Epoch 79/100\n",
      "6602/6602 [==============================] - 29s 4ms/sample - loss: 0.0599 - accuracy: 0.9518\n",
      "Epoch 80/100\n",
      "6602/6602 [==============================] - 29s 4ms/sample - loss: 0.0602 - accuracy: 0.9505\n",
      "Epoch 81/100\n",
      "6602/6602 [==============================] - 30s 5ms/sample - loss: 0.0589 - accuracy: 0.9538\n",
      "Epoch 82/100\n",
      "6602/6602 [==============================] - 29s 4ms/sample - loss: 0.0603 - accuracy: 0.9494\n",
      "Epoch 83/100\n",
      "6602/6602 [==============================] - 29s 4ms/sample - loss: 0.0580 - accuracy: 0.9520\n",
      "Epoch 84/100\n",
      "6602/6602 [==============================] - 29s 4ms/sample - loss: 0.0544 - accuracy: 0.9535\n",
      "Epoch 85/100\n",
      "6602/6602 [==============================] - 29s 4ms/sample - loss: 0.0575 - accuracy: 0.9543\n",
      "Epoch 86/100\n",
      "6602/6602 [==============================] - 30s 4ms/sample - loss: 0.0665 - accuracy: 0.9443\n",
      "Epoch 87/100\n",
      "6602/6602 [==============================] - 29s 4ms/sample - loss: 0.0596 - accuracy: 0.9523\n",
      "Epoch 88/100\n",
      "6602/6602 [==============================] - 29s 4ms/sample - loss: 0.0618 - accuracy: 0.9503\n",
      "Epoch 89/100\n",
      "6602/6602 [==============================] - 29s 4ms/sample - loss: 0.0584 - accuracy: 0.9527\n",
      "Epoch 90/100\n",
      "6602/6602 [==============================] - 29s 4ms/sample - loss: 0.0590 - accuracy: 0.9494\n",
      "Epoch 91/100\n",
      "6602/6602 [==============================] - 29s 4ms/sample - loss: 0.0518 - accuracy: 0.9594\n",
      "Epoch 92/100\n",
      "6602/6602 [==============================] - 29s 4ms/sample - loss: 0.0533 - accuracy: 0.9583\n",
      "Epoch 93/100\n",
      "6602/6602 [==============================] - 29s 4ms/sample - loss: 0.0512 - accuracy: 0.9567\n",
      "Epoch 94/100\n",
      "6602/6602 [==============================] - 29s 4ms/sample - loss: 0.0528 - accuracy: 0.9571\n",
      "Epoch 95/100\n",
      "6602/6602 [==============================] - 29s 4ms/sample - loss: 0.0508 - accuracy: 0.9577\n",
      "Epoch 96/100\n",
      "6602/6602 [==============================] - 29s 4ms/sample - loss: 0.0507 - accuracy: 0.9586\n",
      "Epoch 97/100\n",
      "6602/6602 [==============================] - 30s 5ms/sample - loss: 0.0480 - accuracy: 0.9618\n",
      "Epoch 98/100\n",
      "6602/6602 [==============================] - 29s 4ms/sample - loss: 0.0449 - accuracy: 0.9644\n",
      "Epoch 99/100\n",
      "6602/6602 [==============================] - 29s 4ms/sample - loss: 0.0453 - accuracy: 0.9615\n",
      "Epoch 100/100\n",
      "6602/6602 [==============================] - 29s 4ms/sample - loss: 0.0471 - accuracy: 0.9618\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1b9fabd7f98>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt = tf.keras.optimizers.Adam(lr=0.01, epsilon=1)# learning rate and epsilon are the same as paper DeepLOB\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "model.fit(train_X, train_y, epochs=100, batch_size=32, class_weight=class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1781: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: my_model_ethbtc_v1.1_2021-01-16_2194535408\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save('my_model_ethbtc_v1.1_2021-01-16_2194535408')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
