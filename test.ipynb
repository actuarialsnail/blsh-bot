{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\" # select which GPU(s) to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.0\n",
      "3.8.8\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import mixed_precision\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "import itertools\n",
    "print(tf.__version__)\n",
    "\n",
    "from platform import python_version\n",
    "print(python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK\n",
      "Your GPU will likely run quickly with dtype policy mixed_float16 as it has compute capability of at least 7.0. Your GPU: GeForce RTX 3060, compute capability 8.6\n"
     ]
    }
   ],
   "source": [
    "# Setting the dtype policy for tensor cores\n",
    "mixed_precision.set_global_policy('mixed_float16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "assert len(physical_devices) > 0, \"Not enough GPU hardware devices available\"\n",
    "try:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "    assert tf.config.experimental.get_memory_growth(physical_devices[0])\n",
    "except:\n",
    "    # Invalid device or cannot modify virtual devices once initialized.\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = './datasets/train/'\n",
    "validate_dir = './datasets/validate/'\n",
    "datasample_period = 300\n",
    "prediction_period = 30\n",
    "feature_columns = 40\n",
    "band_size = 0.001\n",
    "capacity_const = 6 * 40 * 65000 # calibration depends on single test run\n",
    "n_train_files = len(os.listdir(train_dir))\n",
    "n_validate_files = len(os.listdir(validate_dir))\n",
    "steps_per_epoch = 0 # to be determined from generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare a directory to store all the checkpoints.\n",
    "checkpoint_dir = './models/checkpoint'\n",
    "if not os.path.exists(checkpoint_dir):\n",
    "    os.makedirs(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = []\n",
    "for subdir, dirs, files in os.walk(validate_dir):\n",
    "    files=files\n",
    "all_arrays = []\n",
    "for npfile in files:\n",
    "    all_arrays.append(np.load(os.path.join(validate_dir, npfile)))\n",
    "np.save('binance_dateset_all.npy', np.concatenate(all_arrays))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.load('./binance_dateset_all.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, data, sample_size, prediction_period, feature_num, band_size, batch_size = 1500, shuffle=True):\n",
    "        'Initialization'\n",
    "        self.data = data\n",
    "        self.sample_size = sample_size\n",
    "        self.prediction_period = prediction_period\n",
    "        self.feature_num = feature_num\n",
    "        self.band_size = band_size\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.data) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        npy_data = self.data[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        \n",
    "        # Generate data\n",
    "        X, y, s = self.__data_generation(npy_data)\n",
    "\n",
    "        return X, y, s\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.data)\n",
    "    \n",
    "    def __data_generation(self, data):\n",
    "        # generate X, Y\n",
    "        shape = data.shape\n",
    "        X = np.zeros((shape[0]-self.sample_size, self.sample_size, self.feature_num), dtype=np.float16)\n",
    "        Y = np.zeros(shape=(shape[0]-self.sample_size, 1), dtype=np.int)\n",
    "        for i in range(shape[0]-self.sample_size):\n",
    "            # take the first feature_num columns as features\n",
    "            X[i] = data[i:i+self.sample_size, 1:self.feature_num+1]\n",
    "            delta_last = (data[i+self.sample_size-1, 0] - data[i, 0]) / data[i+self.sample_size-1, 0]\n",
    "            if delta_last < -band_size:\n",
    "                Y[i] = 0\n",
    "            elif delta_last > band_size:\n",
    "                Y[i] = 2\n",
    "            else:\n",
    "                Y[i] = 1\n",
    "        # add the 4th dimension: 1 channel\n",
    "        X = X.reshape(X.shape[0], self.sample_size, self.feature_num, 1)\n",
    "\n",
    "        # calculate sample_weights for Y\n",
    "        sample_weights_y = np.append(Y.flatten(), [0,1,2]) # to ensure exhaustive coverage\n",
    "        sample_weights_categories = class_weight.compute_class_weight('balanced', classes = np.unique(sample_weights_y), y=sample_weights_y)\n",
    "        idx = 0\n",
    "        sample_weights = np.zeros(shape[0]-self.sample_size)\n",
    "        for y in Y.flatten(): \n",
    "            sample_weights[idx] = sample_weights_categories[y]\n",
    "            idx += 1\n",
    "\n",
    "        # transform y to categorical arrays\n",
    "        y_labels = to_categorical(sample_weights_y)[:-3]\n",
    "                \n",
    "        return X, y_labels, sample_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_generator = DataGenerator(train_data, datasample_period, prediction_period, feature_columns, band_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(directory, sample_size, prediction_period, feature_num, band_size, v=False):\n",
    "    for subdir, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith((\".npy\")):\n",
    "                # data = np.load(os.path.join(subdir, file))[:temp]\n",
    "                data = np.load(os.path.join(subdir, file))\n",
    "\n",
    "                # filter files that are too small\n",
    "                if v: print('processing', file)\n",
    "                if data.shape[0] < sample_size:\n",
    "                    if v: print(file, 'skipped due to insufficient size')\n",
    "                    continue\n",
    "                    \n",
    "                # generate X, Y\n",
    "                shape = data.shape\n",
    "                X = np.zeros((shape[0]-sample_size, sample_size, feature_num), dtype=np.float16)\n",
    "                Y = np.zeros(shape=(shape[0]-sample_size, 1), dtype=np.int)\n",
    "                for i in range(shape[0]-sample_size):\n",
    "                    # take the first feature_num columns as features\n",
    "                    X[i] = data[i:i+sample_size, 1:feature_num+1]\n",
    "                    delta_last = (data[i+sample_size-1, 0] - data[i, 0]) / data[i+sample_size-1, 0]\n",
    "                    if delta_last < -band_size:\n",
    "                        Y[i] = 0\n",
    "                    elif delta_last > band_size:\n",
    "                        Y[i] = 2\n",
    "                    else:\n",
    "                        Y[i] = 1\n",
    "                # add the 4th dimension: 1 channel\n",
    "                X = X.reshape(X.shape[0], sample_size, feature_num, 1)\n",
    "                \n",
    "                # calculate sample_weights for Y\n",
    "                sample_weights_y = np.append(Y.flatten(), [0,1,2]) # to ensure exhaustive coverage\n",
    "                sample_weights_categories = class_weight.compute_class_weight('balanced', classes = np.unique(sample_weights_y), y=sample_weights_y)\n",
    "                idx = 0\n",
    "                sample_weights = np.zeros(shape[0]-sample_size)\n",
    "                for y in Y.flatten(): \n",
    "                    sample_weights[idx] = sample_weights_categories[y]\n",
    "                    idx += 1\n",
    "\n",
    "                # transform y to categorical arrays\n",
    "                y_labels = to_categorical(sample_weights_y)[:-3]\n",
    "                \n",
    "                # stratify into batches\n",
    "                total_size = np.prod(X.shape)\n",
    "                batch_size = math.floor(total_size / capacity_const) + 1\n",
    "                batches_x = np.array_split(X, batch_size)\n",
    "                batches_y = np.array_split(y_labels, batch_size)\n",
    "                batches_sample_weights = np.array_split(sample_weights, batch_size)\n",
    "                for n in range(batch_size):\n",
    "                    yield batches_x[n], batches_y[n], batches_sample_weights[n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#batch = generate_data(train_dir, datasample_period, prediction_period, feature_columns, band_size, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3135 steps per epoch identified\n"
     ]
    }
   ],
   "source": [
    "# calculate how many steps per epoch required\n",
    "steps_per_epoch = 3135 #steps per epoch identified 2021-03-15\n",
    "#steps_per_epoch = sum(1 for dummy in batch)\n",
    "print(steps_per_epoch, 'steps per epoch identified')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = next(batch)\n",
    "# dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(datasample_period, feature_columns):\n",
    "    input_tensor = Input(shape=(datasample_period,feature_columns,1))\n",
    "\n",
    "    # convolutional filter is (1,2) with stride of (1,2)\n",
    "    layer_x = layers.Conv2D(16, (1,2), strides=(1,2))(input_tensor)\n",
    "    layer_x = layers.LeakyReLU(alpha=0.01)(layer_x)\n",
    "    layer_x = layers.Conv2D(16, (4,1), padding='same')(layer_x)\n",
    "    layer_x = layers.LeakyReLU(alpha=0.01)(layer_x)\n",
    "    layer_x = layers.Conv2D(16, (4,1), padding='same')(layer_x)\n",
    "    layer_x = layers.LeakyReLU(alpha=0.01)(layer_x)\n",
    "\n",
    "    layer_x = layers.Conv2D(16, (1,2), strides=(1,2))(layer_x)\n",
    "    layer_x = layers.LeakyReLU(alpha=0.01)(layer_x)\n",
    "    layer_x = layers.Conv2D(16, (4,1), padding='same')(layer_x)\n",
    "    layer_x = layers.LeakyReLU(alpha=0.01)(layer_x)\n",
    "    layer_x = layers.Conv2D(16, (4,1), padding='same')(layer_x)\n",
    "    layer_x = layers.LeakyReLU(alpha=0.01)(layer_x)\n",
    "\n",
    "    layer_x = layers.Conv2D(16, (1,10))(layer_x)\n",
    "    layer_x = layers.LeakyReLU(alpha=0.01)(layer_x)\n",
    "    layer_x = layers.Conv2D(16, (4,1), padding='same')(layer_x)\n",
    "    layer_x = layers.LeakyReLU(alpha=0.01)(layer_x)\n",
    "    layer_x = layers.Conv2D(16, (4,1), padding='same')(layer_x)\n",
    "    layer_x = layers.LeakyReLU(alpha=0.01)(layer_x)\n",
    "\n",
    "    # Inception Module\n",
    "    tower_1 = layers.Conv2D(32, (1,1), padding='same')(layer_x)\n",
    "    tower_1 = layers.LeakyReLU(alpha=0.01)(tower_1)\n",
    "    tower_1 = layers.Conv2D(32, (3,1), padding='same')(tower_1)\n",
    "    tower_1 = layers.LeakyReLU(alpha=0.01)(tower_1)\n",
    "\n",
    "    tower_2 = layers.Conv2D(32, (1,1), padding='same')(layer_x)\n",
    "    tower_2 = layers.LeakyReLU(alpha=0.01)(tower_2)\n",
    "    tower_2 = layers.Conv2D(32, (5,1), padding='same')(tower_2)\n",
    "    tower_2 = layers.LeakyReLU(alpha=0.01)(tower_2)  \n",
    "\n",
    "    tower_3 = layers.MaxPooling2D((3,1), padding='same', strides=(1,1))(layer_x)\n",
    "    tower_3 = layers.Conv2D(32, (1,1), padding='same')(tower_3)\n",
    "    tower_3 = layers.LeakyReLU(alpha=0.01)(tower_3)\n",
    "\n",
    "    layer_x = layers.concatenate([tower_1, tower_2, tower_3], axis=-1)\n",
    "\n",
    "    # concatenate features of tower_1, tower_2, tower_3\n",
    "    layer_x = layers.Reshape((datasample_period,96))(layer_x)\n",
    "\n",
    "    # 64 LSTM units\n",
    "    layer_x = LSTM(64)(layer_x)\n",
    "    # The last output layer uses a softmax activation function\n",
    "    # output = layers.Dense(3, activation='softmax')(layer_x)\n",
    "    x = layers.Dense(3)(x)\n",
    "    output = layers.Activation('softmax', dtype='float32')(x)\n",
    "    output = layers.Activation('linear', dtype='float32')(output)\n",
    "    \n",
    "    model = Model(input_tensor, output)\n",
    "\n",
    "    model.summary()\n",
    "    model.initial_epoch = 0\n",
    "    opt = tf.keras.optimizers.Adam(lr=0.01, epsilon=1) # learning rate and epsilon are the same as paper DeepLOB\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_or_restore_model(datasample_period,feature_columns):\n",
    "    # Either restore the latest model, or create a fresh one\n",
    "    # if there is no checkpoint available.\n",
    "    checkpoints = [checkpoint_dir + '/' + name\n",
    "                   for name in os.listdir(checkpoint_dir)]\n",
    "    if checkpoints:\n",
    "        latest_checkpoint = max(checkpoints, key=os.path.getctime)\n",
    "        print('Restoring from', latest_checkpoint)\n",
    "        model = load_model(latest_checkpoint)\n",
    "        print(latest_checkpoint)\n",
    "        model.initial_epoch = int(latest_checkpoint[latest_checkpoint.index(\"epoch=\")+6:])\n",
    "        return model\n",
    "    print('Creating a new model')\n",
    "    return make_model(datasample_period,feature_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restoring from ./models/checkpoint/ckpt-loss=0.51-epoch=0103\n",
      "./models/checkpoint/ckpt-loss=0.51-epoch=0103\n"
     ]
    }
   ],
   "source": [
    "model = make_or_restore_model(datasample_period,feature_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    # This callback saves a SavedModel every 100 batches.\n",
    "    # We include the training loss in the folder name.\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath=checkpoint_dir + '/ckpt-loss={loss:.2f}-epoch={epoch:04d}/',\n",
    "        save_freq='epoch', save_weights_only=False)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generate_data(train_dir, datasample_period, prediction_period, feature_columns, band_size)\n",
    "t1 = itertools.cycle(train_generator)\n",
    "validate_generator = generate_data(validate_dir, datasample_period, prediction_period, feature_columns, band_size)\n",
    "v1 = itertools.cycle(validate_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 104/200\n",
      "2710/2710 [==============================] - 824s 301ms/step - loss: 0.4800 - accuracy: 0.7905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn, lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn, lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./models/checkpoint/ckpt-loss=0.48-epoch=0104\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./models/checkpoint/ckpt-loss=0.48-epoch=0104\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 105/200\n",
      "  10/2710 [..............................] - ETA: 13:23 - loss: 1.8610 - accuracy: 0.3683"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-54d917536def>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# model.fit(train_generator, epochs=10, steps_per_epoch=n_train_files, validation_data=validate_generator, validation_steps=n_validate_files, callbacks=callbacks, initial_epoch=initial_epoch)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# model.fit(t1, epochs=200, steps_per_epoch=steps_per_epoch, validation_data=v1, validation_steps=n_validate_files, callbacks=callbacks, initial_epoch=initial_epoch)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data_generator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\.conda\\envs\\tf_2_4\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1098\u001b[0m                 _r=1):\n\u001b[0;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1100\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1101\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tf_2_4\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 828\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"xla\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tf_2_4\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    853\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    854\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 855\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    856\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    857\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tf_2_4\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2940\u001b[0m       (graph_function,\n\u001b[0;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2942\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   2943\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   2944\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tf_2_4\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1916\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1917\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1918\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1919\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\.conda\\envs\\tf_2_4\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    553\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    554\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 555\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    556\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    557\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tf_2_4\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "initial_epoch = model.initial_epoch\n",
    "# model.fit(train_generator, epochs=10, steps_per_epoch=n_train_files, validation_data=validate_generator, validation_steps=n_validate_files, callbacks=callbacks, initial_epoch=initial_epoch)\n",
    "# model.fit(t1, epochs=200, steps_per_epoch=steps_per_epoch, validation_data=v1, validation_steps=n_validate_files, callbacks=callbacks, initial_epoch=initial_epoch)\n",
    "model.fit(train_data_generator, epochs=200, callbacks=callbacks, initial_epoch=initial_epoch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
